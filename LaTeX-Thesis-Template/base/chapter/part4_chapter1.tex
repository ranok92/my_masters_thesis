\subsection{Structure of the section}
Overview stating the goals we want to convey though the experiment section. The what and how of the experiments.
\subsubsection*{Overview}
In this chapter we perform various qualitative and quantitative evaluations to showcase the performance of our method compared to others. In the first half we pitch our IRL method to a classical agent and a RL based agent trained in similar setting and a simple yet dense hand crafted reward function. Feature representations play a vital role in an IRL pipeline and so we follow it up by comparing the performance of IRL agents trained in similar setting but different feature representations.
Next, we check the generalizability of our setup and compare the performance of our agent between scenes on which it was trained and scenes on which it wasnt trained (real-life videos as well as custom scenarios)
We understand that getting tricky situations in general from realworld video might be difficult and indeed with the time and effort spent scrolling through the videos to find these are hard, so we also generate some synthetic scenarios to check some of the specific, more advanced qualitative behaviours of the IRL and how that differs to that of the RL.

Lastly, we look into the reward function: the second, and unfortunately, in most of the cases ignored part to of the equation. We try to visualize and interpret the reward function in an attempt to get a better insight into the functioning of our agent.

\subsubsection*{Description of the scenario}

\subsubsection*{Description of the training parameters?}

\subsubsection*{Description of the other feature extractors}
\subsubsection*{Description of the metrics used}
What do we want to show? 
\begin{enumerate}
	\item Justify the use of IRL over RL or traditional methods.
	\item Establish the superiority of the feature representation over other
	\item Show that the smoothing affects the performance of the agent in a positive way.
	\item Show the generalizability of the method. (Compare the performance metrics of the agent in the two different scenarios.)
	\item Another benefit of IRL is the availability of the reward network. Visualization and comprehension of the reward network. 
\end{enumerate}
How do we show?

To test the generalization capabilities of our method we train on the expert demonstrations from lone of the video and test it on data from different scenarios.

\begin{itemize}
        \item Training and testing on the same annotation file.
        \item Training and testing on different annotation files.
        \item Testing on custom scenarios.
\end{itemize}

For each of the test videos above we compare the following metrics:
\begin{itemize}
        \item \textbf{Reaching the goal:} The ability to reach a goal from a given position is one of the most basic and important criteria to measure the performance of a navigating agent. It is calculated as the percentage of runs per 
        \item \textbf{Distance to displacement ratio:} This metric captures the efficiency of the path an agent takes to move between two points. It is calculated as the ratio between the euclidian distance between the two points and the distance traveled by the agent to move between the two. So, for two agents moving from the same start and endpoints, given both of them are successful in reaching the goal, the agent taking a more direct path, is rated better than the other. (The results are shown in the form of histogram plots)
        \item \textbf{Minimum distance over time graphs:} Indicates the minimum distance maintained by an agent throughout its entire trajectory. It can be thought of as a measure of how 'dangerously' or 'cautiously' an agent behaves while interacting with neighboring obstacles. (line graphs over time frame)
        \item \textbf{Avg smoothness:} Trajectories traced by people are smooth with rare occurrences of drastic change in the heading direction. This metric measures how much an agent changes its heading direction, thus the smoothness of its trajectory while negotiating obstacles or navigating in general. (bar plots with error bars.)
        \item \textbf{Drift analysis plots: }This performs a direct comparison between the trajectory taken by an agent and the trajectory followed by the pedestrian, and is calculated as the MSE between the points on the trajectory of the agent and the pedestrian(ground truth)at each time frame. It is a measure of how much the trajectory traced by an agent conforms to the trajectory traced by the actual pedestrian when subjected to similar conditions. (line graphs with error bars)
        \item \textbf{Traced trajectory of multiple agents for a particular pedestrian:} Primarily a visual tool to see how an agent performs in comparison to the ground truth.
\end{itemize}
Participating agents:
\begin{itemize}
        \item Potential fields.
        \item RL DroneFeatures
        \item IRL DroneFeatures Non smoothing
        \item IRL DroneFeatures Smoothing
        \item Vasquez features 1,2, and 3
        \item Fahad features.
\end{itemize}
Visualization of the cost maps at different time frames.
