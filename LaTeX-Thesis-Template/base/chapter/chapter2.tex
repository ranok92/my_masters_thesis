\section{Data driven methods}
In this section, we will go through a different learning-based approach that has been explored in the literature. Learning-based methods in the field of social navigation can be broadly classified into reinforcement learning (RL)-based approach and inverse reinforcement learning(IRL)-based approaches. While these methods differ at their core, the problem definition for both the cases is the same with a key difference. For both RL and IRL settings, the problem at hand is expressed in the form of a Markov decision process (MDP). We define the problem below.
\subsubsection*{Problem definition:}
\textbf{A Markov decision process is a discrete time stochastic control proces$s^{*taken\ from\ wiki}$} and can be represented as a 4 tuple
($\mathcal{S}$,$\mathcal{A}$,T,$\gamma$, $\mathcal{R}$), where,
\begin{itemize}
	\item $\mathcal{S}$ is the set of all possible states.
	\item $\mathcal{A}$ is the set of all possible actions.
	\item T is the transition dynamics.
	\item $\gamma$ is the discount factor.
	\item $\mathcal{R}$ is the set of rewards.
\end{itemize}  
In a reinforcement learning problem setting, the goal is to find a policy $\pi$, which is a function that maps a state to action that maximizes the expected reward obtained.\\
In contrast, in an inverse reinforcement learning problem setting, there is no reward function provided. Instead, we are provided with a set of expert demonstrations say D = \{$\tau_1$, $\tau_2$, ... \}. And the goal is to generate a reward function $\mathcal{R}$, that best explains the expert behavior and a policy that would maximize the reward function.


\subsection*{RL based approaches}
 In their work, \textbf{Socially aware motion planning with Deep RL, Chen et. al.} address this problem by focusing on what not to do, rather than what to do. They introduce a reward function crafted to induce social norms in the behavior of the agent.
The reward function primarily focuses on three different aspects of interaction: overtaking, passing and crossing. The reward they define is as follows:
Equation 9
They note that symmetry is one of the driving factors to incorporate social behavior, so they introduce a network architecture for symmetric multi-agent training.
A picture of the symmetric network.

\textbf{Collision avoidance in pedestrian-rich environments with deep reinforcement learning}
Addresses the drawback of the previous work, the inability to tackle variable number of agents in the environment. They address this by introducing a Long Short Term Memory (LSTM) based network architecture that takes as input the information from the nearby pedestrians in a sequence. This eliminates the need for specifying a limit on the number of nearby-agents the method can handle.\\
The state comprises of two parts: 
\begin{enumerate}
	\item The information of the agent itself, \textbf{s}
	\item The information from the other n number of agents present in the environment at a given time. This is where the LSTM comes in to play which takes the information from these n agents and passes them sequentially through an LSTM layer to generate a final hidden state which is concatenated with \textbf{s} to obtain the final state representation of the environment.
\end{enumerate}
\textbf{Pictures of the network architecture}
\\
The reward function used in this work is a relatively straight forward sparse reward structure with positive and negative feedback for reaching the goal or encountering a collision and \textbf{a reward function decay in near-collision states}.  

\textbf{Conclusion}\\
Reinforcement learning, as a class of methods, has been widely successful in solving various complex control tasks including but not limited to video games making it one of the more obvious choices to tackle the problem of social navigation. 
However, using RL in the task of social navigation necessitates the formulation of a reward function that can correctly capture the 'social and cultural' characteristics. Coming up with such a set of rules can be difficult and daunting as 'social norms' are not always explicit and can vary widely across different societies and even based on a given situation.\\

And so, the application of inverse RL seems almost too tempting to let go. A quick browse 
\subsection*{IRL based approaches}
\textbf{Learning approaches meet probabilistic road map style path planner.}

\textbf{Vasquez et al} in their work, present a comprehensive study on the effect of different feature representations and IRL methods on the performance of an agent in the task of social navigation. They examine two IRL methods:
max-margin IRL and max entropy IRL.\\
For the feature representations, they create a pool of measurements garnered from the state of the agent itself and the state of the other entities in the environment and combine individual entities to create 3 feature representations. \\
They test all of these on a ROS based pedestrian simulator on 3 scenarios: airport gate, crossing hallway, and intersection. Expert demonstrations for these scenarios are obtained through teleoperation. They find that the performance across the two learning methods are similar. The feature representation, on the other hand, plays a major role in the final performance of the agent hence they conclude that one should spend more time and effort building better feature representations or come up with learning methods that aid in the simplification of building the feature representations.\\

\textbf{Kim and pineau} use a similar idea, where they use expert demonstrations of controlling a wheelchair and use that to create a socially compliant agent for controlling a wheelchair which they take a step further by installing in a real wheelchair. \\
In one of the more recent works by \textbf{Fahad et. al.}, they present an inverse RL pipeline to train agents from demonstrations using maximum entropy IRL.
The objective is to maximize the likelihood of observing the states visited by the expert, which is expressed as Equation 3.
Each demonstration, or trajectory, in this case, is effectively broken into a sum of the individual states encountered by the expert. The final update rule for the reward network is given by equation 6 
where $\mu_{d}$ is the state visitation frequency is the expected value of the states visited by the expert and $E[\mu]$ is the expected value of states visited by the agent. The reward function in the algorithm is also directly dependent on the state and so, the way the states are interpreted using feature representations becomes a vital part of the research in social navigation and IRL in general. 
In most of the cases, the goal is to find succinct yet meaningful features that can capture the information needed by an agent to properly comprehend its current state. Good representation can adequately capture significant bits of information needed to become the expert without being too bulky.  The feature representation used here can be broken into 4 parts:
Social Affinity Map feature(SAM): This captures the motion of the pedestrians in the vicinity of the agent. The area near the agent is divided into two concentric circles. The region inside the inner circle is then divided into 4 parts, and the region between the inner and outer circle is divided into 6 parts as shown in the figure below.
Information from each of these 10 areas (or bins) is then expressed using a 6-dimensional vector:
THE VECTOR	
that captures the average velocity of the pedestrians in a given bin. The feature thresholds used are given below.

2.Density feature: Helps provide an idea of how dense the vicinity of the agent is. It is calculated by adding up the number of pedestrians present in the spatial bins (calculated above). Once calculated they are discretized based on some predefined threshold.
Distance feature: Captures the distance between the agent's current location and the goal.
Default Cost feature: Introduced to balance the rest of the other features. This has been proposed by various other works in the past.
The final feature representation is given by 

And the overall method is given by algorithm 1.


Conclusion:
Both, reinforcement learning and inverse reinforcement learning bring a unique set of strengths and limitations to the table when it comes to the tackling of the problem of social navigation. 

For inverse reinforcement learning:
Adv:
The primary advantage of IRL over all other methods discussed, both model-based and data-driven methods, is that there is no need to specify a handcrafted reward function built to induce certain kinds of behavior within the agent. Most of the current work in this domain primarily focuses on capturing the expert's behavior, thus encapsulating the 'naturalness' in the movements during navigation.
Disadvantages:
Comment of IRL and feature engineering. 
Getting expert demonstrations are more expensive compared to RL.
Hard to generalize. 
For RL:
In RL based work, the 'socialness' of a learning agent can and is introduced by carefully designed reward functions. In SA-CADRL, the authors use a handcrafted reward function to introduce the right-handed rules during scenarios like passing, crossing and overtaking, focusing primarily on introducing the 'naturalness' factor in the agent's policy.
Disadv:
Handcrafting rewards can help cover some aspects of the way a human navigates, but given that it is hard to figure out and generalize what is the social norm for different social interactions (of which there are many) sets RL based methods to a heavy limitation.