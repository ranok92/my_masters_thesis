In this work, we present a data-driven IRL-based social navigation pipeline. An overview of the pipeline:


We will describe each component of the pipeline in greater detail.
\section*{The IRL block:}
Inverse reinforcement learning(IRL) or inverse optimal control(IOC) has been in vogue in recent years when it comes to training robots to perform real-world tasks. This is understandable as assigning rewards to individual states to illicit out desired behaviors is challenging. IRL provides a better alternative of obtaining the underlying reward function as well as a trained agent on the obtained reward function using demonstrations from the expert.

To keep the chapter self-contained, we will briefly go over the definition of a Markov decision process, which is at the base of reinforcement and inverse reinforcement learning.
A Markov decision process or MDP can be defined as a tuple {S, A, P, R, gamma} where,
S is the set of all possible states,
A is the set of all possible actions,
P is the state transition matrix,
R is the reward and gamma is the discount factor.
As stated by Ziebart in their paper 2008, the objective of maxent IRL is to find a reward function that maximizes the probability of the states visited by the expert while maximizing the entropy of the model. The idea behind this is, select that reward function which, assigns similar rewards to states that have not been visited by the expert. This helps in reducing ambiguity in the reward function.

The objective function.
Going through the math, the objective function boils down to the minimization of the difference between the SVF of the agent and the expert.

The original paper by Ziebart used a linear combination of weights with the feature representation of the states to obtain a reward. Linear representation falls short when trying to express reward functions that cannot be expressed in linear combinations. This problem is addressed by Wulfmeier 2015 where they restructure the maxent IRL method using neural networks (universal function approximators).

Change in the math due to swapping of the linear functions with neural networks.


In their work, Wulfmeier assumed a model-based environment and used the state transition matrix to obtain the SVF of an agent, we find that a fairly generous assumption, as capturing the state dynamics of real-world environments can be quite challenging. Instead, we focus on calculating the SVF using sampling-based methods. The details on the calculation of the SVF is present in the next section. 

\subsection*{Overview of the algorithm used}
The algorithm trains for two networks, the reward network that, given the features of a state returns the reward associated with it,\\
\textbf{equation}\\ stating this.
and the policy network, which given the same, returns the best possible action.\\
\textbf{equation}\\
The method starts with randomly initializing the weights of the reward network. This reward network is then used in the  RL block to train an agent which is optimal for the current reward structure. Once, an optimal policy is obtained, the policy is then sampled from, in the environment to obtain roll outs or trajectories in this case. A trajectory is given by the sequence of states visited by the agent {s1, s2, ... sn}.
Once the trajectories are obtained, they are used to calculate the state visitation frequency. The difference between the expert and the agent SVF is used to calculate the loss
\textbf{equation}
This loss is then back propagated through the reward network to update the weights.
Once the weights are updated, the new network is again fed into the RL block. This iterative process continues until completion.
Explanation of the L1 regularization over l2 regularization 

\section*{The RL block:}
Theory of actor-critic method. A type of policy gradient method. Policy gradient methods are a class of reinforcement learning methods where the policy is iteratively improved.


\section*{The feature extractor}:
The feature extractor is a vital component in the navigation pipeline as it acts as the medium using which the learning algorithm interacts with the environment something that provides the agent with the information/context to act upon. 
We have tested with different feature representations and have found that it plays a vital role, as the feature representations provide the information to the agent on which we want them to base their decision on.

The feature extractor can primarily be broken down into 2 broad components: local information and global information. The local information, as the name suggests, uses the information from the nearby surroundings of the agent and captures that in a binary feature vector. This provides an approximate idea of the obstacles in the vicinity. The global information provides a rough direction of where the goal of the agent is. We believe that this provides the agent with a purpose of navigation, preventing it from just rambling around the map while the local information is used to avoid obstacles and maintain proper decorum while moving towards the goal.
Although the environment publishes all the information that is there to know about the obstacles in the map, including their location, orientation and velocity, and also the exact coordinates of the goal location, we recognize that getting access to this kind of information in real life on a mobile robot navigating any given scene in the real world is extremely hard to obtain. 
This is again addressed by the feature extractor, which simultaneously acts as an information moderator, sorting information from the environment and presenting it in such a way that it can realistically be obtained by a mobile robot trying to navigate in the real world.
Both the local and the global components along with their subcomponents are described in greater detail below.
Talk about the relative orientation calculation


\subsection*{The global information}
The global information is further comprised of 3 elements:
The relative goal orientation: 
This acts as a compass, providing a rough estimate of the location of the goal based on the current position and orientation of the agent. This is denoted by a 9x1 one-hot vector, where the presence of the goal in any one of the bins is marked by a '1' keeping the rest to '0'. The 360 degrees around the agent are divided into 8 equal divisions forming the first 8 bins. The 9th bin denotes the contact of the agent with the goal. The structuring of the bins is shown below. 
The aforementioned way of getting the relative orientation concerning the agent is employed to get the relative orientation of various entities and not just the goal as we will see in subsequent sections.

The change in orientation: Represented by a 4x1 one-hot vector, the change in orientation captures the magnitude of the change in the orientation of the agent in consecutive steps. The entire range of [0-180] is divided asymmetrically into 4 divisions. The rationale behind the uneven distribution is that empirically we have found that when humans move,  they do not tend to change their orientation drastically. And having a finer resolution in the lower range help capture the nuances in this behavior in greater detail leading to better encapsulation of the essence of the navigational pattern. The division of the range is shown below.

The deviation from goal: Represented by a 4x1 one-hot vector, the deviation from goal captures the magnitude of the angle between the vector to the goal from the current position of the agent and the current orientation vector of the agent. This value can range from 0-180 which is equally divided into 4 bins as shown below.

\subsection*{The local information}
The purpose of the local information is to cram is as much information as possible in the most succinct way. Taking inspiration from previous works in this field, we have used spatial bins to effectively break the region surrounding the agent into discrete parts.

Creation of the bins:
There are two layers of spatial bins surrounding the agent and is denoted by constructing two concentric circles around the agent. The region between the agent and the inner circle is then broken into 8 equal divisions and they comprise of bin 1-8. Similarly, the region between the first and the second bin is again divided into 8 equal divisions which form bins 9-16.
Calculation of the risk:
'Risk' is a term we concoct to measure much of a  threat the agent is facing at a given time from any of these bins. Here 'threat' can loosely be seen as a measure of the possibility of hitting an obstacle. The higher the threat, the greater the chance that if the agent and the obstacles continue on their current course it will end in a collision.
The 'risk' can be seen in 3 levels:
high low and something in between.
High risk:
When the relative motion of an obstacle is towards the agent.
Low risk:
When the relative motion of an obstacle is away from the agent.
Med risk:
If not any of the above, it is classified as medium risk.
Mathematically, they are calculated as follows:

relative orient = obs orientation - agent orientation
relative distance = agent position - obs position
Let the angle between the two vectors is theta, then it is classified as high risk when
theta is less than 90 and tan theta * relative distance < threshold
where the threshold depends on the radius of the agent and that of the obstacle at hand.
if theta > 90, that means the obstacle is moving away from the agent, and hence chances of collision with the agent are highly unlikely resulting in the classification of low risk.

One thing to note is the risk is calculated for individual obstacles present in the bin separately. And it is not uncommon to have more than one obstacle falling in different risk divisions from the same spatial bin. In that case, the risk value assigned to that bin is the highest risk posed among all the obstacles that fall under that spatial bin.

\subsection*{The SVF calculation}:
Calculation of the agent SVF:
Calculation of the expert SVF:
Challenges of Inverse reinforcement learning in the task of social navigation:



