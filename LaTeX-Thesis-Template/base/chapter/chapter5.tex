\textbf{General description of the environment:}\\
This chapter presents a detailed description of the environment used for running the experiments. The environment portrays the world as a rectangular area viewed from a top-down perspective (bird's eye view). It adopts the template of a regular OpenAI gym environment and is built using PyGame.
The size of the world can be set by altering the rows and columns during initialization. These values denote the size of the world in pixels.

The environment follows the image coordinate system: with the center at the top left corner and each position on the map is indicated by a 2-dimensional vector containing the row and column value respectively.\\
\section{Components}
3 key components populate the environment: 
\begin{itemize}
    \item The agent.
    \item The goal.
    \item The obstacles.
\end{itemize}
\subsection{The agent}
The agent represents the entity being controlled by an external controller. At any given moment, the state of the agent has three properties: position, speed, and orientation.
\begin{enumerate}
    \item Position: The current coordinates of the agent (in pixel coordinates) on the map.
    \item Speed: The current speed of the agent.
    \item Orientation: A unit vector along the velocity vector of the agent.
\end{enumerate}
To navigate the environment the agent can control two aspects of its motion: its speed and its orientation.
Unlike most stock environments, this supports both continuous and discrete action space. These are elaborated in \autoref{subsec:action-space}.
\subsubsection{The goal:}
The goal is a pre-designated area of the map where on arrival marks the end of an episode with a success. It is represented by a green rectangle. Like the agent, the size of the goal area can be customized as per the requirement of the experiment. For now, the environment just has provision for static goals. That being said, the environment does come with the provision to randomly alter the position of the goal at the end of each episode.


\subsection{The obstacles:}
There are 3 ways of putting the obstacles in the map each of them serving a different purpose.
\begin{enumerate}
\item \textbf{Via a list:}
The information for the placement of the obstacles can be passed in the form of a list, where each element of the list is a Numpy array containing the pixel coordinate ([row, col]) of the obstacle. This is primarily useful when the number of obstacles to be deployed is small. Although it severely limits the number and the configuration of the obstacles that can be placed in the map, it is ideal for performing a quick initialization or a brief sanity test of a controller in a short time without investing in the making an obstacle map or an annotation file.
\item \textbf{Via an obstacle map:}
This is the primary way of initializing the environment with static obstacles. In this case, the path to an image file is provided during initialization. This image is then read by the environment to obtain the size of the map and the obstacle configuration. 
The dimensions of the image determine the size of the map and are equal to the image. Comprehending the obstacle configuration is also straight forward. Any red pixel, pixel falling in the color range of (150, 0, 0) to (255, 0, 0) is categorized as an obstacle. While defining each pixel as an individual obstacle renders the most accurate recreation of the obstacle configuration from a given image, it is inefficient as it generates an unnecessarily large number of obstacles on the map.
This is redundant as for most of the cases, instead of having pixel-sized obstacles,  these obstacles can be grouped into bigger cells often without loosing too much information or diverging too much from the original configuration. The grouping can be adjusted by tuning the obstacle-width parameter in the environment.  Setting the width to one sets the size of an individual obstacle to 1. This retains the highest amount of information (basically pixel-by-pixel information) from the obstacle map. Increasing the width groups pixels in the image to form obstacles of the given width, which drastically reduces the number of obstacles at the cost of introducing a pixelation effect in the final obstacle map.
%\textbf{A set of 3 images: obstacle image, and its comparison with changing obstacle size}
\begin{figure}[!h]
	\begin{subfigure}[b]{.5\textwidth}
		\centering
		\includegraphics[width=.95\linewidth]{figures/real_map.jpg}
		\caption{The original obstacle map}
		\label{fig:obsmap_sfig1}
	\end{subfigure}%
	\begin{subfigure}[b]{.5\textwidth}
		\centering
		\includegraphics[width=.95\linewidth]{figures/obs_width_2_total_obs_51348.jpg}
		\caption{Obstacle width 2, number of obstacles}
		\label{fig:obsmap_sfig2}
	\end{subfigure}%
	\hfill
	\begin{subfigure}[b]{.5\textwidth}
		\centering
		\includegraphics[width=.95\linewidth]{figures/obs_width_7_total_obs_4293.jpg}
		\caption{Obstacle width 7, number of obstacles: 4293}
		\label{fig:obsmap_sfig3}
	\end{subfigure}
	\begin{subfigure}[b]{.5\textwidth}
		\centering
		\includegraphics[width=.95\linewidth]{figures/obs_width_15_total_obs_996.jpg}
		\caption{Obstacle width 15, number of obstacles: 996}
		\label{fig:obsmap_sfig4}
\end{subfigure}
	\caption{Recreation of the given image in the form of an obstacle map in the environment in different resolutions.}
	\label{fig:fig}
\end{figure}
\item \textbf{Via an annotation file.}
This is used to populate the environment with dynamic obstacles. The annotation file is a text file containing frame-by-frame information of all the obstacles and is commonly used to recreate pedestrian motion captured in the real-world. An annotation file should adhere to this particular format: 4 columns frame, id, y-coordinate, and x-coordinate, in that order, and separated by blank spaces.
\begin{itemize}
    \item frame id: The serial number of a given frame, with respect to the first frame.
    \item id: A unique id assigned to each of the obstacles in the video. This helps in keeping track of the obstacles from one frame to the next.
    \item y-coordinate: The position of the obstacle in column space.
    \item x-coordinate: The position of the obstacle in row space.
\end{itemize}
\begin{figure}[htbp]
	\begin{subfigure}{0.5\textwidth}
		\centering
		\includegraphics[width=.95\linewidth]{figures/video_frame_students.jpg}
		\caption{Frame from the original video}
		\label{fig:anno_sfig1}
	\end{subfigure}
	\begin{subfigure}{0.5\textwidth}
		\centering
		\includegraphics[width=0.95\linewidth]{figures/env_screenshot3.jpg}
		\caption{Corresponding frame recreated in the environment using the annotation file.}
		\label{fig:anno_sfig2}
	\end{subfigure}
\end{figure}
\end{enumerate}
\subsection{Input spaces and observation}
\label{subsec:action-space}
The environment has provisions for both discrete and continuous action space.
\begin{enumerate}
    \item \textbf{The continuous action space:} is implemented using the Box class from OpenAI gym. The agent can take an action in the form of a 2-dimensional vector which contains the change in the speed and orientation. This, along with the information from the current state of the agent is used to compute the position, velocity, and orientation of the agent for the next state as shown in \autoref{eq:next_state_agent} and \autoref{eq:next_state_agent2}.\\
   \begin{align}
	\nabla_{orient} & = \texttt{action}_{0}\\
	\nabla_{speed} & = \texttt{action}_{1}
	\end{align}
    where $S_{max}$ is the maximum speed of the agent and $\nabla_{orient}$ and $\nabla_{speed}$ are the change in orientation and speed at a given step respectively.
    
    \item \textbf{For discrete action space:} the action space is implemented using the Discrete class from OpenAI gym. The rate of change in speed and orientation is divided into 7 and 5 bins respectively thus allowing the agent to take one out of the 35 actions at its disposal to interact with the environment. In the discrete action space, \textcolor{red}{Some sort of graphical/tabular representation of the action spaces}
    Although the underlying control is similar, in the discrete action space, the control action is provided in the form of  a single scalar value and the change in speed and orientation is retrieved from it as follows:\\
    \begin{align}
	    \nabla_{orient} & = \texttt{Arr}_{orient}[\texttt{act}_{t}\%7]\\
	    \nabla_{speed} & = \texttt{Arr}_{speed}[\texttt{act}_{t}/7]   
    \end{align}
    where $\texttt{act}_{t} \in \mathbb{Z} [ 0, 35]$, $\texttt{Arr}_{orient}$ is the orientation array and, $\texttt{Arr}_{speed}$ is the speed array.\\
    The speed and orientation of the agent in the next step is calculated as:
    \begin{align}
		\label{eq:next_state_agent}
		\texttt{speed}_{t+1} & =\texttt{min\{} \texttt{max\{(}\texttt{speed}_{t} + \texttt{action}[0]\texttt{), 0\}, } S_{max}\}\\
		\label{eq:next_state_agent2}
		\texttt{orient}_{t+1} & = (\texttt{orient}_{t} + \texttt{action}[1]) \% 360
	\end{align}
\begin{table}[tbhp]
	\caption{Bins}
	\label{tab:env_action_divisions}
	\begin{center}
		\begin{tabular}{|c|c|c|c|}
			\hline
			Motion component & Lower bound & Upper bound & Divisions \\
			\hline
			\hline
			Speed & $-0.4$ &  $+0.4$ & 5 \\
			Orientation & $30^{\circ}$ anti-clockwise & $30^{\circ}$ clockwise & 7\\
			\hline
		\end{tabular}
	\end{center}
\end{table}
\end{enumerate}
\subsubsection{The Observation:}
The observation from the environment also follows the format of a standard gym environment returning a tuple of 4 elements: state, reward, done-flag, and info.
The state:
The state publishes information relating to the agent, goal, and the obstacles in the form of a dictionary.
The reward:
The reward obtained at a given time step.
The done-flag:
A flag that indicates the completion of an episode.
Info:
\subsection{Additional features}
In addition to the hallmarks of a regular gym environment, the environment comes with a set of options that provide added flexibility and customizations while training and evaluating IRL agents.
\subsubsection{Training modes:}
The environment supports 3 types of training modes:. 
\begin{enumerate}
    \item \textbf{Fixed respawn:} The starting and goal position of the agent remains the same throughout the training process.
    \item \textbf{Random respawn:} For every episode, the agent and the goal spawn at random positions on the map. 
    \item \textbf{Replace a pedestrian:} For every episode, the agent assumes the role of a randomly selected pedestrian. Once a pedestrian is selected, the pedestrian is ignored as an obstacle for that episode. The initial position of the agent is set to the coordinated of the pedestrian in its first frame of appearance. Similarly, the goal is set to the coordinates of the pedestrian in the final frame.
    This mode is especially useful for training IRL models because demonstrations from experts for navigation are the optimal demonstrations for a particular configuration of goal and obstacle. Placing the agent in the exact context enables the agent to observe the same state distribution as the expert, which helps to maintain the effectiveness of the expert demonstrations. 
\end{enumerate}
Features available for testing of the environment:
As this environment was designed primarily for the development of IRL methods for navigation, it also comes with some built-in tools to test IRL methods.
\subsubsection{Deployment of the ghost}
We know, IRL agents in a way learn from demonstrations. And creating meaning metrics to evaluate their performance is elusive, which is why we resort to using IRL in the first place. One simple and effective way of conducting a performance analysis would be to subject the agent to the same conditions as the expert and compare and contrast their behaviors. This is exactly what the environment facilitates. This shadow or 'ghost' does not in any way impact the state of the agent and can be used for both qualitative and quantitative analysis of the performance of the agent in comparison to the actual expert. 

\subsubsection{User control}
The environment also has a provision to let an external user control and agent and navigate the crowds. There might be cases where there are not enough expert demonstrations available to perform proper training. In that case, the size of the expert demonstration set can be bolstered by letting humans take control of the agent and generate more expert demonstrations. 

The agent can be controlled by a mouse pointer and action is registered only at the time of the click of the left mouse button. Once a click is registered on the map, the direction and magnitude of the vector joining the current position and the location of the click are taken as the desired orientation and speed respectively.  
The orientation action is then calculated based on the difference between the current orientation of the agent and comparing that with the desired direction. If there is a difference between the two, and a change in the current heading direction of the agent is warranted, then the action to inflict the change is then decided based on the magnitude and sign of the change needed relative to the current heading direction of the agent to minimize the difference.
The speed action is also calculated in a similar manner, where the magnitude of the difference vector is treated as the desired speed, and based on the current speed of the agent action is taken to minimize the difference.

\subsubsection{Creating custom scenarios:}
The environment has provisions to create custom dynamic scenarios, where the motion of each of the pedestrians can be predefined. This is a very handy tool for creating and testing the agents in situations that are hard to come across or simply unavailable in the available datasets.\\
\textbf{Any need for elaboration on this?}


